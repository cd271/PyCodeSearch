{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be8687ce-bd43-46be-b222-fd90dc910c4d",
   "metadata": {},
   "source": [
    "This notebook demonstrates the full process of `SemanticCodeSearch` using fine-tuned GraphCodeBERT model, which implement the code-to-code search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67703c6-7e3a-4b39-8c76-090f2d7affff",
   "metadata": {
    "id": "Eh1yfA0-Dt-f"
   },
   "source": [
    "### Download test repositories and run `inspect4py` on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d58ea6-3276-4b02-a712-8bc8b0576c45",
   "metadata": {
    "id": "_G--A3sT61LR"
   },
   "outputs": [],
   "source": [
    "# Repository picked from https://github.com as an example\n",
    "repo = 'keon/algorithms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3483ca-fadc-4d9f-a437-5ec5a458d5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/Documents/Project/Examples/RepoAnalysis/CodeSearch/Code2code/content\n",
      "Cloning into 'keon/algorithms'...\n",
      "remote: Enumerating objects: 5162, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
      "remote: Total 5162 (delta 11), reused 16 (delta 3), pack-reused 5135\u001b[K\n",
      "Receiving objects: 100% (5162/5162), 1.42 MiB | 10.46 MiB/s, done.\n",
      "Resolving deltas: 100% (3227/3227), done.\n",
      "Updating files: 100% (477/477), done.\n",
      "Creating jsDir:output/keon/algorithms/algorithms/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/docs/source/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/streaming/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/json_files\n",
      "Error when processing invert_tree.py:  <class 'AttributeError'>\n",
      "Error when processing deepest_left.py:  <class 'AttributeError'>\n",
      "Error when processing longest_consecutive.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/avl/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/traversal/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/trie/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/red_black_tree/json_files\n",
      "Error when processing red_black_tree.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/bst/json_files\n",
      "Error when processing kth_smallest.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/segment_tree/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/fenwick_tree/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/compression/json_files\n",
      "Added in funct/method elias_generic , argument named unary, number of argument 0\n",
      "Added in funct/method elias_generic , argument named elias_gamma, number of argument 0\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/ml/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/map/json_files\n",
      "Error when processing hashtable.py:  <class 'AttributeError'>\n",
      "Error when processing separate_chaining_hashtable.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/dfs/json_files\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Error when processing pacific_atlantic.py:  <class 'AttributeError'>\n",
      "Error when processing walls_and_gates.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/greedy/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/queues/json_files\n",
      "Error when processing queue.py:  <class 'AttributeError'>\n",
      "Error when processing priority_queue.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/search/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/stack/json_files\n",
      "Error when processing stack.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/maths/json_files\n",
      "Error when processing polynomial.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/dp/json_files\n",
      "Error when processing longest_increasing.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unionfind/json_files\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/backtrack/json_files\n",
      "Error when processing generate_abbreviations.py:  <class 'AttributeError'>\n",
      "Error when processing add_operators.py:  <class 'AttributeError'>\n",
      "Error when processing generate_parenthesis.py:  <class 'AttributeError'>\n",
      "Error when processing find_words.py:  <class 'AttributeError'>\n",
      "Error when processing palindrome_partitioning.py:  <class 'AttributeError'>\n",
      "Error when processing array_sum_combinations.py:  <class 'AttributeError'>\n",
      "Error when processing combination_sum.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/strings/json_files\n",
      "Error when processing word_squares.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/distribution/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unix/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unix/path/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/matrix/json_files\n",
      "Error when processing search_in_sorted_matrix.py:  <class 'AttributeError'>\n",
      "Error when processing sum_sub_squares.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/heap/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/set/json_files\n",
      "Error when processing randomized_set.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/linkedlist/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/bfs/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/automata/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/graph/json_files\n",
      "Error when processing path_between_two_vertices_in_digraph.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/sort/json_files\n",
      "Error when processing stooge_sort.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/bit/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/arrays/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/tests/json_files\n",
      "Error when processing test_monomial.py:  <class 'AttributeError'>\n",
      "Error when processing test_polynomial.py:  <class 'AttributeError'>\n",
      "Analysis completed\n",
      "Total number of folders processed (root folder is considered a folder): 37\n",
      "Total number of files found:  378\n",
      "Total number of classes found:  269\n",
      "Total number of dependencies found in those files 919\n",
      "Total number of functions parsed:  523\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p content/output\n",
    "%cd content/\n",
    "\n",
    "!mkdir -p {repo} && git clone {f\"https://github.com/{repo}.git\"} {repo}\n",
    "!inspect4py -i {repo} -o output/{repo} -sc -rm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21286e-b5d7-47f6-8c73-cd509a458fc3",
   "metadata": {
    "id": "tXCNJRtRKQXP"
   },
   "source": [
    "### Extract docstrings and functions from repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33cf8e0-0e08-4ed9-b857-585eb6e75cb1",
   "metadata": {
    "id": "F-89vQxSFR4s"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def funcs_to_lists(funcs, func_codes, docs):\n",
    "    for func_name, func_info in funcs.items():\n",
    "        if func_info.get(\"source_code\") is not None:\n",
    "            func_codes.append(func_info[\"source_code\"])\n",
    "        if func_info.get(\"doc\") is None:\n",
    "            continue\n",
    "        for key in [\"full\", \"long_description\", \"short_description\"]:\n",
    "            if func_info[\"doc\"].get(key) is not None:\n",
    "                docs.append(f\"{func_name} {func_info['doc'].get(key)}\")\n",
    "                break\n",
    "\n",
    "def file_to_lists(filename):\n",
    "    func_codes = []\n",
    "    docs = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    dic.pop(\"readme_files\", None)\n",
    "    for dir_name, files in dic.items():\n",
    "        for file in files:\n",
    "            if file.get(\"functions\") is not None:\n",
    "                funcs_to_lists(file[\"functions\"], func_codes, docs)\n",
    "            if file.get(\"classes\") is not None:\n",
    "                for class_name, class_info in file[\"classes\"].items():\n",
    "                    if class_info.get(\"methods\") is not None:\n",
    "                        funcs_to_lists(class_info[\"methods\"], func_codes, docs)\n",
    "    return func_codes, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0f0d9b-143b-4ffb-82e7-7437c10cf042",
   "metadata": {
    "id": "zw0cKz1nKXE1"
   },
   "outputs": [],
   "source": [
    "repo_info = {}\n",
    "function_list, docstring_list = file_to_lists(f\"output/{repo}/directory_info.json\")\n",
    "repo_info[\"docs\"] = docstring_list\n",
    "repo_info[\"funcs\"] = function_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fec977-a8eb-4174-bbaa-2c631b18a442",
   "metadata": {
    "id": "FdvlATGULNoX"
   },
   "source": [
    "### Download GraghCodeBERT fine-tuned model and using pipeline to calculate code similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f197b8-43fc-413d-85f7-ce85df7631ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/codesearch/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"Lazyhope/python-clone-detection\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f6ab26-a9eb-4a78-b120-c5c811a9db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "def test_topsort(self):\n",
    "    res_recursive = top_sort_recursive(self.depGraph)\n",
    "    self.assertTrue(res_recursive.index('g') < res_recursive.index('e'))\n",
    "    \n",
    "    res_iterative = top_sort(self.depGraph)\n",
    "    self.assertTrue(res_iterative.index('g') < res_iterative.index('e'))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729fdb97-d137-494d-bb93-cc65ea9c784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_index(lst, n):\n",
    "    largest_indices = []\n",
    "    for i in range(n):\n",
    "        max_value = max(lst)\n",
    "        max_index = lst.index(max_value)\n",
    "        largest_indices.append(max_index)\n",
    "        lst[max_index] = float('-inf')\n",
    "    return largest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cae16030-e74a-4205-b605-445518cc9fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating similarities: 100%|███████████████████████████████████████████████████████████████████| 1171/1171 [01:36<00:00, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9999922513961792, \n",
      "def test_topsort(self):\n",
      "    res = top_sort_recursive(self.depGraph)\n",
      "    self.assertTrue(res.index('g') < res.index('e'))\n",
      "    res = top_sort(self.depGraph)\n",
      "    self.assertTrue(res.index('g') < res.index('e')) \n",
      "------------------------------------------------------------------\n",
      "\n",
      "Similarity: 0.9981995820999146, \n",
      "def setUp(self):\n",
      "    self.depGraph = {'a': ['b'], 'b': ['c'], 'c': ['e'], 'e': ['g'], 'd': [], 'f': ['e', 'd'], 'g': []} \n",
      "------------------------------------------------------------------\n",
      "\n",
      "Similarity: 0.9960083961486816, \n",
      "def test_is_sorted(self):\n",
      "    head = Node(-2)\n",
      "    head.next = Node(2)\n",
      "    head.next.next = Node(2)\n",
      "    head.next.next.next = Node(4)\n",
      "    head.next.next.next.next = Node(9)\n",
      "    self.assertTrue(is_sorted(head))\n",
      "    head = Node(1)\n",
      "    head.next = Node(2)\n",
      "    head.next.next = Node(8)\n",
      "    head.next.next.next = Node(4)\n",
      "    head.next.next.next.next = Node(6)\n",
      "    self.assertFalse(is_sorted(head)) \n",
      "------------------------------------------------------------------\n",
      "\n",
      "Similarity: 0.9959540367126465, \n",
      "def test_palindromic_substrings(self):\n",
      "    string1 = 'abc'\n",
      "    answer1 = [['a', 'b', 'c']]\n",
      "    self.assertEqual(palindromic_substrings(string1), sorted(answer1))\n",
      "    string2 = 'abcba'\n",
      "    answer2 = [['abcba'], ['a', 'bcb', 'a'], ['a', 'b', 'c', 'b', 'a']]\n",
      "    self.assertEqual(sorted(palindromic_substrings(string2)), sorted(answer2))\n",
      "    string3 = 'abcccba'\n",
      "    answer3 = [['abcccba'], ['a', 'bcccb', 'a'], ['a', 'b', 'ccc', 'b', 'a'], ['a', 'b', 'cc', 'c', 'b', 'a'], ['a', 'b', 'c', 'cc', 'b', 'a'], ['a', 'b', 'c', 'c', 'c', 'b', 'a']]\n",
      "    self.assertEqual(sorted(palindromic_substrings(string3)), sorted(answer3)) \n",
      "------------------------------------------------------------------\n",
      "\n",
      "Similarity: 0.9951157569885254, \n",
      "def test_3x4(self):\n",
      "    grid1 = [['0', 'E', '0', '0'], ['E', '0', 'W', 'E'], ['0', 'E', '0', '0']]\n",
      "    self.assertEqual(3, bomb_enemy.max_killed_enemies(grid1))\n",
      "    grid1 = [['0', 'E', '0', 'E'], ['E', 'E', 'E', '0'], ['E', '0', 'W', 'E'], ['0', 'E', '0', '0']]\n",
      "    grid2 = [['0', '0', '0', 'E'], ['E', '0', '0', '0'], ['E', '0', 'W', 'E'], ['0', 'E', '0', '0']]\n",
      "    self.assertEqual(5, bomb_enemy.max_killed_enemies(grid1))\n",
      "    self.assertEqual(3, bomb_enemy.max_killed_enemies(grid2)) \n",
      "------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "similarities = []\n",
    "for func in tqdm(repo_info[\"funcs\"], desc=\"Calculating similarities\"):\n",
    "    compare_dict = dict(pipe((query, func)))\n",
    "    similarities.append(compare_dict[True])\n",
    "\n",
    "sim = similarities.copy()\n",
    "index = find_top_n_index(sim,5)\n",
    "for i in index:\n",
    "    print(f'Similarity: {similarities[i]}, \\n{repo_info[\"funcs\"][i]} \\n------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2208a6-9edc-47c8-9b78-e666cf896f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
