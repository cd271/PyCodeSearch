{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada5442b-9101-4292-988f-5c4fb1e919b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = 'keon/algorithms'\n",
    "\n",
    "import json\n",
    "\n",
    "def funcs_to_lists(funcs, func_codes, docs):\n",
    "    for func_name, func_info in funcs.items():\n",
    "        if func_info.get(\"source_code\") is not None:\n",
    "            func_codes.append(func_info[\"source_code\"])\n",
    "        if func_info.get(\"doc\") is None:\n",
    "            continue\n",
    "        for key in [\"full\", \"long_description\", \"short_description\"]:\n",
    "            if func_info[\"doc\"].get(key) is not None:\n",
    "                docs.append(f\"{func_name} {func_info['doc'].get(key)}\")\n",
    "                break\n",
    "\n",
    "def file_to_lists(filename):\n",
    "    func_codes = []\n",
    "    docs = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    dic.pop(\"readme_files\", None)\n",
    "    for dir_name, files in dic.items():\n",
    "        for file in files:\n",
    "            if file.get(\"functions\") is not None:\n",
    "                funcs_to_lists(file[\"functions\"], func_codes, docs)\n",
    "            if file.get(\"classes\") is not None:\n",
    "                for class_name, class_info in file[\"classes\"].items():\n",
    "                    if class_info.get(\"methods\") is not None:\n",
    "                        funcs_to_lists(class_info[\"methods\"], func_codes, docs)\n",
    "    return func_codes\n",
    "    \n",
    "repo_info = {}\n",
    "function_list = file_to_lists(f\"content/output/{repo}/directory_info.json\")\n",
    "repo_info[\"funcs\"] = function_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313dfb27-bd9f-48ab-b30a-ca21f0a63ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/codesearch/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"clda/codebert-python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2661d909-e479-4bdd-8cbf-b0b32617002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AutoModel.from_pretrained(\"clda/graphcodebert-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e6ea35-51c3-42ec-9c3b-9896dffa1a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating code embeddings for dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/1171 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1171/1171 [01:22<00:00, 14.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset code embeddings generated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Generating code embeddings for dataset ... \")\n",
    "code_embeddings = []\n",
    "for func in tqdm(repo_info[\"funcs\"]):\n",
    "    code_embeddings.append(model2(tokenizer(func,return_tensors='pt', max_length=512)['input_ids'])[1])\n",
    "    \n",
    "print(\"Dataset code embeddings generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3f4e1f-5bd7-43a5-bb8b-736b734af807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2045482/727774269.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  code_embeddings = torch.stack([torch.tensor(embedding) for embedding in code_embeddings])\n"
     ]
    }
   ],
   "source": [
    "code_embeddings = torch.stack([torch.tensor(embedding) for embedding in code_embeddings])\n",
    "\n",
    "code_vecs = torch.squeeze(code_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e96995d6-f680-4e8b-9994-600c1ea9c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Function to calcualte cosine similarity\n",
      "Code: def cosine_similarity(vec1, vec2):\n",
      "    \"\"\"\n",
      "    Calculate cosine similarity between given two vectors\n",
      "    :type vec1: list\n",
      "    :type vec2: list\n",
      "    \"\"\"\n",
      "    if len(vec1) != len(vec2):\n",
      "        raise ValueError('The two vectors must be the same length. Got shape ' + str(len(vec1)) + ' and ' + str(len(vec2)))\n",
      "    norm_a = _l2_distance(vec1)\n",
      "    norm_b = _l2_distance(vec2)\n",
      "    similarity = 0.0\n",
      "    for (vec1_element, vec2_element) in zip(vec1, vec2):\n",
      "        similarity += vec1_element * vec2_element\n",
      "    similarity /= norm_a * norm_b\n",
      "    return similarity\n",
      "Similarity: 0.9495675563812256\n",
      "Code: def test_cosine_similarity(self):\n",
      "    vec_a = [1, 1, 1]\n",
      "    vec_b = [-1, -1, -1]\n",
      "    vec_c = [1, 2, -1]\n",
      "    self.assertAlmostEqual(cosine_similarity(vec_a, vec_a), 1)\n",
      "    self.assertAlmostEqual(cosine_similarity(vec_a, vec_b), -1)\n",
      "    self.assertAlmostEqual(cosine_similarity(vec_a, vec_c), 0.4714045208)\n",
      "Similarity: 0.0504324734210968\n",
      "Code: def scc(graph):\n",
      "    \"\"\" Computes the strongly connected components of a graph \"\"\"\n",
      "    order = []\n",
      "    visited = {vertex: False for vertex in graph}\n",
      "    graph_transposed = {vertex: [] for vertex in graph}\n",
      "    for (source, neighbours) in graph.iteritems():\n",
      "        for target in neighbours:\n",
      "            add_edge(graph_transposed, target, source)\n",
      "    for vertex in graph:\n",
      "        if not visited[vertex]:\n",
      "            dfs_transposed(vertex, graph_transposed, order, visited)\n",
      "    visited = {vertex: False for vertex in graph}\n",
      "    vertex_scc = {}\n",
      "    current_comp = 0\n",
      "    for vertex in reversed(order):\n",
      "        if not visited[vertex]:\n",
      "            dfs(vertex, current_comp, vertex_scc, graph, visited)\n",
      "            current_comp += 1\n",
      "    return vertex_scc\n",
      "Similarity: 6.6453789138165575e-09\n"
     ]
    }
   ],
   "source": [
    "query = \"Function to calcualte cosine similarity\"\n",
    "query_vec = model2(tokenizer(query,return_tensors='pt')['input_ids'])[1]\n",
    "\n",
    "scores=torch.einsum(\"ab,cb->ac\",query_vec,code_vecs)\n",
    "scores=torch.softmax(scores,-1)\n",
    "\n",
    "top_scores, top_indices = torch.topk(scores[0], k=3, largest=True)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "for score, idx in zip(top_scores, top_indices):\n",
    "    code_embedding = repo_info[\"funcs\"][idx]\n",
    "    print(\"Code:\", code_embedding)\n",
    "    print(\"Similarity:\", score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd990e-7825-4243-b002-7577860a29da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
