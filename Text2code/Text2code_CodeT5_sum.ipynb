{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be8687ce-bd43-46be-b222-fd90dc910c4d",
   "metadata": {},
   "source": [
    "This notebook demonstrates the full process of `SemanticCodeSearch` using fine-tuned GraphCodeBERT model, which implement the code-to-code search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d67703c6-7e3a-4b39-8c76-090f2d7affff",
   "metadata": {
    "id": "Eh1yfA0-Dt-f"
   },
   "source": [
    "### Download test repositories and run `inspect4py` on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d58ea6-3276-4b02-a712-8bc8b0576c45",
   "metadata": {
    "id": "_G--A3sT61LR"
   },
   "outputs": [],
   "source": [
    "# Repository picked from https://github.com as an example\n",
    "repo = 'keon/algorithms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c7bc956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect4py, version 0.0.6\n"
     ]
    }
   ],
   "source": [
    "!inspect4py --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbd97db-607a-4d11-a24e-bc2bf59e7432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/Documents/Project/Examples/RepoAnalysis/SemanticCodeSearch/Text2code/content\n",
      "Cloning into 'keon/algorithms'...\n",
      "remote: Enumerating objects: 5162, done.\u001b[K\n",
      "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 5162 (delta 11), reused 16 (delta 3), pack-reused 5136\u001b[K\n",
      "Receiving objects: 100% (5162/5162), 1.42 MiB | 10.99 MiB/s, done.\n",
      "Resolving deltas: 100% (3231/3231), done.\n",
      "Updating files: 100% (477/477), done.\n",
      "Creating jsDir:output/keon/algorithms/algorithms/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/streaming/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/map/json_files\n",
      "Error when processing separate_chaining_hashtable.py:  <class 'AttributeError'>\n",
      "Error when processing hashtable.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/stack/json_files\n",
      "Error when processing stack.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/dfs/json_files\n",
      "Error when processing pacific_atlantic.py:  <class 'AttributeError'>\n",
      "Error when processing walls_and_gates.py:  <class 'AttributeError'>\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/dp/json_files\n",
      "Error when processing longest_increasing.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/search/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/queues/json_files\n",
      "Error when processing priority_queue.py:  <class 'AttributeError'>\n",
      "Error when processing queue.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/greedy/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/json_files\n",
      "Error when processing longest_consecutive.py:  <class 'AttributeError'>\n",
      "Error when processing deepest_left.py:  <class 'AttributeError'>\n",
      "Error when processing invert_tree.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/segment_tree/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/trie/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/traversal/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/fenwick_tree/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/bst/json_files\n",
      "Error when processing kth_smallest.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/avl/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/red_black_tree/json_files\n",
      "Error when processing red_black_tree.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/automata/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/strings/json_files\n",
      "Error when processing word_squares.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/graph/json_files\n",
      "Error when processing path_between_two_vertices_in_digraph.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/set/json_files\n",
      "Error when processing randomized_set.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unionfind/json_files\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/backtrack/json_files\n",
      "Error when processing find_words.py:  <class 'AttributeError'>\n",
      "Error when processing add_operators.py:  <class 'AttributeError'>\n",
      "Error when processing array_sum_combinations.py:  <class 'AttributeError'>\n",
      "Error when processing generate_parenthesis.py:  <class 'AttributeError'>\n",
      "Error when processing palindrome_partitioning.py:  <class 'AttributeError'>\n",
      "Error when processing generate_abbreviations.py:  <class 'AttributeError'>\n",
      "Error when processing combination_sum.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/heap/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unix/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unix/path/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/ml/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/bfs/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/sort/json_files\n",
      "Error when processing stooge_sort.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/bit/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/compression/json_files\n",
      "Added in funct/method elias_generic , argument named unary, number of argument 0\n",
      "Added in funct/method elias_generic , argument named elias_gamma, number of argument 0\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/linkedlist/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/arrays/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/distribution/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/matrix/json_files\n",
      "Error when processing search_in_sorted_matrix.py:  <class 'AttributeError'>\n",
      "Error when processing sum_sub_squares.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/maths/json_files\n",
      "Error when processing polynomial.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/tests/json_files\n",
      "Error when processing test_monomial.py:  <class 'AttributeError'>\n",
      "Error when processing test_polynomial.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/docs/source/json_files\n",
      "Analysis completed\n",
      "Total number of folders processed (root folder is considered a folder): 37\n",
      "Total number of files found:  378\n",
      "Total number of classes found:  269\n",
      "Total number of dependencies found in those files 919\n",
      "Total number of functions parsed:  523\n",
      "/cs/home/cd271/Documents/Project/Examples/RepoAnalysis/SemanticCodeSearch/Text2code\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p content/output\n",
    "%cd content/\n",
    "\n",
    "!mkdir -p {repo} && git clone {f\"https://github.com/{repo}.git\"} {repo}\n",
    "!inspect4py -i {repo} -o output/{repo} -sc -rm\n",
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c21286e-b5d7-47f6-8c73-cd509a458fc3",
   "metadata": {
    "id": "tXCNJRtRKQXP"
   },
   "source": [
    "### Extract docstrings and functions from repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33cf8e0-0e08-4ed9-b857-585eb6e75cb1",
   "metadata": {
    "id": "F-89vQxSFR4s"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def funcs_to_lists(funcs, func_codes, docs):\n",
    "    for func_name, func_info in funcs.items():\n",
    "        if func_info.get(\"source_code\") is not None:\n",
    "            func_codes.append(func_info[\"source_code\"])\n",
    "        if func_info.get(\"doc\") is None:\n",
    "            continue\n",
    "        for key in [\"full\", \"long_description\", \"short_description\"]:\n",
    "            if func_info[\"doc\"].get(key) is not None:\n",
    "                docs.append(f\"{func_name} {func_info['doc'].get(key)}\")\n",
    "                break\n",
    "\n",
    "def file_to_lists(filename):\n",
    "    func_codes = []\n",
    "    docs = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    dic.pop(\"readme_files\", None)\n",
    "    for dir_name, files in dic.items():\n",
    "        for file in files:\n",
    "            if file.get(\"functions\") is not None:\n",
    "                funcs_to_lists(file[\"functions\"], func_codes, docs)\n",
    "            if file.get(\"classes\") is not None:\n",
    "                for class_name, class_info in file[\"classes\"].items():\n",
    "                    if class_info.get(\"methods\") is not None:\n",
    "                        funcs_to_lists(class_info[\"methods\"], func_codes, docs)\n",
    "    return func_codes, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0f0d9b-143b-4ffb-82e7-7437c10cf042",
   "metadata": {
    "id": "zw0cKz1nKXE1"
   },
   "outputs": [],
   "source": [
    "repo_info = {}\n",
    "function_list, _ = file_to_lists(f\"content/output/{repo}/directory_info.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "974b901a",
   "metadata": {},
   "source": [
    "### Download UniXCoder, fine-tuned model and install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfffb63c-02df-4384-b41c-3eccb6028d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/codesearch/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"Salesforce/codet5-base-multi-sum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d6429a-b851-41c5-88b8-fc313b2585d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_sum(funcs):\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        funcs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Perform inference to get code similarity\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        outputs = model.generate(**inputs)\n",
    "        \n",
    "    similar_code_snippets = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return similar_code_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b8ab2d-f396-4150-937b-210e2021f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code summaries:   0%|                                                               | 0/1171 [00:00<?, ?it/s]/cs/home/cd271/codesearch/lib64/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Generating code summaries: 100%|████████████████████████████████████████████████████| 1171/1171 [06:05<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Obtain function_list summarizations \n",
    "code_sum = []\n",
    "with tqdm(total=len(function_list), desc=\"Generating code summaries\") as pbar:\n",
    "    for funcs in function_list:\n",
    "        code_snippets = get_code_sum([funcs])\n",
    "        code_sum.extend(code_snippets)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2f4c19-ddbe-4a35-a74d-062d7118e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for input query\n",
    "query = ['check if the number in bitsum is vilad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee389947-3c76-465b-b06c-505ca17846fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sum_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n",
    "\n",
    "def get_embedding(code_sum, query_sum):\n",
    "    return sum_model.encode(code_sum, convert_to_tensor=True), sum_model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "code_sum_embeddings, query_sum_embedding = get_embedding(code_sum, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "359299ed-cac2-4cbf-856f-5933fd7f02ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "cosine_sim = CosineSimilarity(dim=1)\n",
    "similarities = cosine_sim(query_sum_embedding, code_sum_embeddings).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522c0424-7d2e-4a9f-9835-770359827e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_index(lst, n):\n",
    "    largest_indices = []\n",
    "    for i in range(n):\n",
    "        max_value = max(lst)\n",
    "        max_index = lst.index(max_value)\n",
    "        largest_indices.append(max_index)\n",
    "        lst[max_index] = float('-inf')\n",
    "    return largest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47fc5780-eb32-49ac-9580-0639897c75bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similiar code snippet:\n",
      "\n",
      "Similarity: 0.8362308740615845, \n",
      "def _check_every_number_in_bitsum(bitsum, sum_signs):\n",
      "    for val in bitsum:\n",
      "        if val != 0 and val != sum_signs:\n",
      "            return False\n",
      "    return True \n",
      "--------------------------------------------------------------\n",
      "Similarity: 0.7320685982704163, \n",
      "def test_get_bit(self):\n",
      "    self.assertEqual(1, get_bit(22, 2))\n",
      "    self.assertEqual(0, get_bit(22, 3)) \n",
      "--------------------------------------------------------------\n",
      "Similarity: 0.653519868850708, \n",
      "def test_has_alternative_bit_fast(self):\n",
      "    self.assertTrue(has_alternative_bit_fast(5))\n",
      "    self.assertFalse(has_alternative_bit_fast(7))\n",
      "    self.assertFalse(has_alternative_bit_fast(11))\n",
      "    self.assertTrue(has_alternative_bit_fast(10)) \n",
      "--------------------------------------------------------------\n",
      "Similarity: 0.649571418762207, \n",
      "def test_has_alternative_bit(self):\n",
      "    self.assertTrue(has_alternative_bit(5))\n",
      "    self.assertFalse(has_alternative_bit(7))\n",
      "    self.assertFalse(has_alternative_bit(11))\n",
      "    self.assertTrue(has_alternative_bit(10)) \n",
      "--------------------------------------------------------------\n",
      "Similarity: 0.6493911743164062, \n",
      "def has_alternative_bit_fast(n):\n",
      "    mask1 = int('aaaaaaaa', 16)\n",
      "    mask2 = int('55555555', 16)\n",
      "    return mask1 == n + (n ^ mask1) or mask2 == n + (n ^ mask2) \n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sim = similarities.copy()\n",
    "index = find_top_n_index(sim,5)\n",
    "print('Similiar code snippet:\\n')\n",
    "for i in index:\n",
    "    print(f'Similarity: {similarities[i]}, \\n{function_list[i]} \\n--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741513a2-caef-4d0e-a742-291e0f6edc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
