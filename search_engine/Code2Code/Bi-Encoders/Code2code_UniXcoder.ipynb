{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "071c5200-c4c5-4696-8760-d0b3785bb4cd",
   "metadata": {},
   "source": [
    "This notebook implement Code-To-Code Search Engine with fine-tuned `UniXcoder` model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d67703c6-7e3a-4b39-8c76-090f2d7affff",
   "metadata": {
    "id": "Eh1yfA0-Dt-f"
   },
   "source": [
    "### Download test repositories and run `inspect4py` on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d58ea6-3276-4b02-a712-8bc8b0576c45",
   "metadata": {
    "id": "_G--A3sT61LR"
   },
   "outputs": [],
   "source": [
    "# Repository picked from https://github.com as an example\n",
    "repo = 'keon/algorithms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c7bc956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect4py, version 0.0.6\n"
     ]
    }
   ],
   "source": [
    "!inspect4py --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9004ae35-bac5-4279-9535-d88e10e53cd9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/Documents/Project/Examples/SemanticCodeSearch/search_engine/Code2Code/Bi-Encoders/content\n",
      "fatal: destination path 'keon/algorithms' already exists and is not an empty directory.\n",
      "Error when processing test_monomial.py:  <class 'AttributeError'>\n",
      "Error when processing test_polynomial.py:  <class 'AttributeError'>\n",
      "Error when processing stooge_sort.py:  <class 'AttributeError'>\n",
      "Error when processing walls_and_gates.py:  <class 'AttributeError'>\n",
      "Error when processing pacific_atlantic.py:  <class 'AttributeError'>\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Error when processing separate_chaining_hashtable.py:  <class 'AttributeError'>\n",
      "Error when processing hashtable.py:  <class 'AttributeError'>\n",
      "Error when processing stack.py:  <class 'AttributeError'>\n",
      "Added in funct/method elias_generic , argument named unary, number of argument 0\n",
      "Added in funct/method elias_generic , argument named elias_gamma, number of argument 0\n",
      "Error when processing search_in_sorted_matrix.py:  <class 'AttributeError'>\n",
      "Error when processing sum_sub_squares.py:  <class 'AttributeError'>\n",
      "Error when processing randomized_set.py:  <class 'AttributeError'>\n",
      "Error when processing path_between_two_vertices_in_digraph.py:  <class 'AttributeError'>\n",
      "Error when processing longest_increasing.py:  <class 'AttributeError'>\n",
      "Error when processing polynomial.py:  <class 'AttributeError'>\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Error when processing array_sum_combinations.py:  <class 'AttributeError'>\n",
      "Error when processing add_operators.py:  <class 'AttributeError'>\n",
      "Error when processing combination_sum.py:  <class 'AttributeError'>\n",
      "Error when processing generate_parenthesis.py:  <class 'AttributeError'>\n",
      "Error when processing generate_abbreviations.py:  <class 'AttributeError'>\n",
      "Error when processing palindrome_partitioning.py:  <class 'AttributeError'>\n",
      "Error when processing find_words.py:  <class 'AttributeError'>\n",
      "Error when processing queue.py:  <class 'AttributeError'>\n",
      "Error when processing priority_queue.py:  <class 'AttributeError'>\n",
      "Error when processing word_squares.py:  <class 'AttributeError'>\n",
      "Error when processing deepest_left.py:  <class 'AttributeError'>\n",
      "Error when processing longest_consecutive.py:  <class 'AttributeError'>\n",
      "Error when processing invert_tree.py:  <class 'AttributeError'>\n",
      "Error when processing kth_smallest.py:  <class 'AttributeError'>\n",
      "Error when processing red_black_tree.py:  <class 'AttributeError'>\n",
      "Analysis completed\n",
      "Total number of folders processed (root folder is considered a folder): 37\n",
      "Total number of files found:  378\n",
      "Total number of classes found:  269\n",
      "Total number of dependencies found in those files 919\n",
      "Total number of functions parsed:  523\n",
      "/cs/home/cd271/Documents/Project/Examples/SemanticCodeSearch/search_engine/Code2Code/Bi-Encoders\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p content/output\n",
    "%cd content/\n",
    "\n",
    "!mkdir -p {repo} && git clone {f\"https://github.com/{repo}.git\"} {repo}\n",
    "!inspect4py -i {repo} -o output/{repo} -sc -rm\n",
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c21286e-b5d7-47f6-8c73-cd509a458fc3",
   "metadata": {
    "id": "tXCNJRtRKQXP"
   },
   "source": [
    "### Extract docstrings and functions from repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33cf8e0-0e08-4ed9-b857-585eb6e75cb1",
   "metadata": {
    "id": "F-89vQxSFR4s"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def funcs_to_lists(funcs, func_codes, docs):\n",
    "    for func_name, func_info in funcs.items():\n",
    "        if func_info.get(\"source_code\") is not None:\n",
    "            func_codes.append(func_info[\"source_code\"])\n",
    "        if func_info.get(\"doc\") is None:\n",
    "            continue\n",
    "        for key in [\"full\", \"long_description\", \"short_description\"]:\n",
    "            if func_info[\"doc\"].get(key) is not None:\n",
    "                docs.append(f\"{func_name} {func_info['doc'].get(key)}\")\n",
    "                break\n",
    "\n",
    "def file_to_lists(filename):\n",
    "    func_codes = []\n",
    "    docs = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    dic.pop(\"readme_files\", None)\n",
    "    for dir_name, files in dic.items():\n",
    "        for file in files:\n",
    "            if file.get(\"functions\") is not None:\n",
    "                funcs_to_lists(file[\"functions\"], func_codes, docs)\n",
    "            if file.get(\"classes\") is not None:\n",
    "                for class_name, class_info in file[\"classes\"].items():\n",
    "                    if class_info.get(\"methods\") is not None:\n",
    "                        funcs_to_lists(class_info[\"methods\"], func_codes, docs)\n",
    "    return func_codes, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0f0d9b-143b-4ffb-82e7-7437c10cf042",
   "metadata": {
    "id": "zw0cKz1nKXE1"
   },
   "outputs": [],
   "source": [
    "repo_info = {}\n",
    "function_list, docstring_list = file_to_lists(f\"content/output/{repo}/directory_info.json\")\n",
    "repo_info[\"funcs\"] = function_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "974b901a",
   "metadata": {},
   "source": [
    "### Download UniXcoder, use fine-tuned UniXcoder model for code NL-NL embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d170f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/microsoft/CodeBERT/master/UniXcoder/unixcoder.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ddb750f",
   "metadata": {},
   "source": [
    "### Generate embeddings for all repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dac3de1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/codesearch/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unixcoder import UniXcoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "code_search_model = UniXcoder(\"Lazyhope/unixcoder-nine-advtest\")\n",
    "clone_detection_model = UniXcoder(\"Lazyhope/unixcoder-clone-detection\")\n",
    "code_search_model.to(device)\n",
    "clone_detection_model.to(device)\n",
    "\n",
    "def get_code_embeddings(code, model):\n",
    "    tokens_ids = model.tokenize([code], max_length=512, mode=\"<encoder-only>\")\n",
    "    source_ids = torch.tensor(tokens_ids).to(device)\n",
    "    _, embeddings = model(source_ids)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a88798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Generating func embeddings for repo - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1171/1171 [01:02<00:00, 18.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Generate code embeddings for all funcs in repository\n",
    "print(f\" - Generating func embeddings for repo - \")\n",
    "code_embeddings = []\n",
    "for func in tqdm(repo_info[\"funcs\"]):\n",
    "    code_embeddings.append(get_code_embeddings(func, clone_detection_model))    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9549b586",
   "metadata": {},
   "source": [
    "### Test & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c27a5a-807e-4e8a-b99c-8e47c0868c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "def bfs(graph, start):\n",
    "    visited = set()\n",
    "    queue = deque([start])\n",
    "    visited.add(start)\n",
    "    result = []\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        result.append(node)\n",
    "\n",
    "        for neighbor in graph.get(node, []):\n",
    "            if neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append(neighbor)\n",
    "    return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9db4a8-1732-432d-8dff-16192516a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "input_embedding = get_code_embeddings(query, clone_detection_model)\n",
    "\n",
    "cosine_sim = CosineSimilarity(dim=1, eps=1e-8)\n",
    "similarities = cosine_sim(input_embedding, torch.stack(code_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff24d59-a4a7-4b4d-900f-231b676b7f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similiar code snippets:\n",
      "\n",
      "------------------------------------------------------------------\n",
      " def dfs_traverse(graph, start):\n",
      "    \"\"\"\n",
      "    Traversal by depth first search.\n",
      "    \"\"\"\n",
      "    (visited, stack) = (set(), [start])\n",
      "    while stack:\n",
      "        node = stack.pop()\n",
      "        if node not in visited:\n",
      "            visited.add(node)\n",
      "            for next_node in graph[node]:\n",
      "                if next_node not in visited:\n",
      "                    stack.append(next_node)\n",
      "    return visited\n",
      "\n",
      "------------------------------------------------------------------\n",
      " def bfs_traverse(graph, start):\n",
      "    \"\"\"\n",
      "    Traversal by breadth first search.\n",
      "    \"\"\"\n",
      "    (visited, queue) = (set(), [start])\n",
      "    while queue:\n",
      "        node = queue.pop(0)\n",
      "        if node not in visited:\n",
      "            visited.add(node)\n",
      "            for next_node in graph[node]:\n",
      "                if next_node not in visited:\n",
      "                    queue.append(next_node)\n",
      "    return visited\n",
      "\n",
      "------------------------------------------------------------------\n",
      " def dinic_bfs(capacity, flow, level, source, sink):\n",
      "    \"\"\"\n",
      "    BFS function for Dinic algorithm.\n",
      "    Check whether sink is reachable only using edges that is not full.\n",
      "    \"\"\"\n",
      "    vertices = len(capacity)\n",
      "    queue = Queue()\n",
      "    queue.put(source)\n",
      "    level[source] = 0\n",
      "    while queue.qsize():\n",
      "        front = queue.get()\n",
      "        for nxt in range(vertices):\n",
      "            if level[nxt] == -1 and flow[front][nxt] < capacity[front][nxt]:\n",
      "                level[nxt] = level[front] + 1\n",
      "                queue.put(nxt)\n",
      "    return level[sink] != -1\n",
      "\n",
      "------------------------------------------------------------------\n",
      " def maze_search(maze):\n",
      "    (BLOCKED, ALLOWED) = (0, 1)\n",
      "    (UNVISITED, VISITED) = (0, 1)\n",
      "    (initial_x, initial_y) = (0, 0)\n",
      "    if maze[initial_x][initial_y] == BLOCKED:\n",
      "        return -1\n",
      "    directions = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
      "    (height, width) = (len(maze), len(maze[0]))\n",
      "    (target_x, target_y) = (height - 1, width - 1)\n",
      "    queue = deque([(initial_x, initial_y, 0)])\n",
      "    is_visited = [[UNVISITED for w in range(width)] for h in range(height)]\n",
      "    is_visited[initial_x][initial_y] = VISITED\n",
      "    while queue:\n",
      "        (x, y, steps) = queue.popleft()\n",
      "        if x == target_x and y == target_y:\n",
      "            return steps\n",
      "        for (dx, dy) in directions:\n",
      "            new_x = x + dx\n",
      "            new_y = y + dy\n",
      "            if not (0 <= new_x < height and 0 <= new_y < width):\n",
      "                continue\n",
      "            if maze[new_x][new_y] == ALLOWED and is_visited[new_x][new_y] == UNVISITED:\n",
      "                queue.append((new_x, new_y, steps + 1))\n",
      "                is_visited[new_x][new_y] = VISITED\n",
      "    return -1\n",
      "\n",
      "------------------------------------------------------------------\n",
      " def move_zeros(array):\n",
      "    result = []\n",
      "    zeros = 0\n",
      "    for i in array:\n",
      "        if i == 0 and type(i) != bool:\n",
      "            zeros += 1\n",
      "        else:\n",
      "            result.append(i)\n",
      "    result.extend([0] * zeros)\n",
      "    return result\n"
     ]
    }
   ],
   "source": [
    "# Convert similarities tensor to a list\n",
    "similarities_list = similarities.tolist()\n",
    "\n",
    "# Combine functions with cosine similarities\n",
    "func_similarities = list(zip(repo_info[\"funcs\"], similarities_list))\n",
    "\n",
    "# Sort the func_similarities list based on cosine similarities\n",
    "sorted_similarities = sorted(func_similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "num_similar_funcs = 5  # Specify the number of similar functions to retrieve\n",
    "most_similar_funcs = sorted_similarities[:num_similar_funcs]\n",
    "\n",
    "# Extract the function names from the most_similar_funcs list\n",
    "similar_func_names = [func for func, _ in most_similar_funcs]\n",
    "\n",
    "# Output the function names\n",
    "print('Similiar code snippets:')\n",
    "for func_name in similar_func_names:\n",
    "    print(f'\\n------------------------------------------------------------------\\n {func_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20201192-dd84-41df-80db-3132dbfa9573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
