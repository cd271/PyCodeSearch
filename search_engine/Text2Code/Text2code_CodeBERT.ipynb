{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0485bfef-121f-40cc-a9df-f41049bd8b31",
   "metadata": {},
   "source": [
    "This notebook implement Text-To-Code Search Engine with `CodeBERT` and `GraphCodeBERT` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c571bf58-fa76-4afb-af26-f7de82548fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/microsoft/CodeBERT/master/UniXcoder/unixcoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d84b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect4py, version 0.0.6\n"
     ]
    }
   ],
   "source": [
    "!inspect4py --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5749d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/Documents/Project/Examples/RepoAnalysis/SemanticCodeSearch/Text2code/content\n",
      "Cloning into 'keon/algorithms'...\n",
      "remote: Enumerating objects: 5162, done.\u001b[K\n",
      "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 5162 (delta 11), reused 16 (delta 3), pack-reused 5136\u001b[K\n",
      "Receiving objects: 100% (5162/5162), 1.42 MiB | 10.99 MiB/s, done.\n",
      "Resolving deltas: 100% (3231/3231), done.\n",
      "Updating files: 100% (477/477), done.\n",
      "Creating jsDir:output/keon/algorithms/algorithms/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/streaming/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/map/json_files\n",
      "Error when processing separate_chaining_hashtable.py:  <class 'AttributeError'>\n",
      "Error when processing hashtable.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/stack/json_files\n",
      "Error when processing stack.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/dfs/json_files\n",
      "Error when processing pacific_atlantic.py:  <class 'AttributeError'>\n",
      "Error when processing walls_and_gates.py:  <class 'AttributeError'>\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/dp/json_files\n",
      "Error when processing longest_increasing.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/search/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/queues/json_files\n",
      "Error when processing priority_queue.py:  <class 'AttributeError'>\n",
      "Error when processing queue.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/greedy/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/json_files\n",
      "Error when processing longest_consecutive.py:  <class 'AttributeError'>\n",
      "Error when processing deepest_left.py:  <class 'AttributeError'>\n",
      "Error when processing invert_tree.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/segment_tree/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/trie/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/traversal/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/fenwick_tree/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/bst/json_files\n",
      "Error when processing kth_smallest.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/avl/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/red_black_tree/json_files\n",
      "Error when processing red_black_tree.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/automata/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/strings/json_files\n",
      "Error when processing word_squares.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/graph/json_files\n",
      "Error when processing path_between_two_vertices_in_digraph.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/set/json_files\n",
      "Error when processing randomized_set.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unionfind/json_files\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/backtrack/json_files\n",
      "Error when processing find_words.py:  <class 'AttributeError'>\n",
      "Error when processing add_operators.py:  <class 'AttributeError'>\n",
      "Error when processing array_sum_combinations.py:  <class 'AttributeError'>\n",
      "Error when processing generate_parenthesis.py:  <class 'AttributeError'>\n",
      "Error when processing palindrome_partitioning.py:  <class 'AttributeError'>\n",
      "Error when processing generate_abbreviations.py:  <class 'AttributeError'>\n",
      "Error when processing combination_sum.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/heap/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unix/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unix/path/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/ml/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/bfs/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/sort/json_files\n",
      "Error when processing stooge_sort.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/bit/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/compression/json_files\n",
      "Added in funct/method elias_generic , argument named unary, number of argument 0\n",
      "Added in funct/method elias_generic , argument named elias_gamma, number of argument 0\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/linkedlist/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/arrays/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/distribution/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/matrix/json_files\n",
      "Error when processing search_in_sorted_matrix.py:  <class 'AttributeError'>\n",
      "Error when processing sum_sub_squares.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/maths/json_files\n",
      "Error when processing polynomial.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/tests/json_files\n",
      "Error when processing test_monomial.py:  <class 'AttributeError'>\n",
      "Error when processing test_polynomial.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/docs/source/json_files\n",
      "Analysis completed\n",
      "Total number of folders processed (root folder is considered a folder): 37\n",
      "Total number of files found:  378\n",
      "Total number of classes found:  269\n",
      "Total number of dependencies found in those files 919\n",
      "Total number of functions parsed:  523\n",
      "/cs/home/cd271/Documents/Project/Examples/RepoAnalysis/SemanticCodeSearch/Text2code\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p content/output\n",
    "%cd content/\n",
    "\n",
    "!mkdir -p {repo} && git clone {f\"https://github.com/{repo}.git\"} {repo}\n",
    "!inspect4py -i {repo} -o output/{repo} -sc -rm\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada5442b-9101-4292-988f-5c4fb1e919b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = 'keon/algorithms'\n",
    "\n",
    "import json\n",
    "\n",
    "def funcs_to_lists(funcs, func_codes, docs):\n",
    "    for func_name, func_info in funcs.items():\n",
    "        if func_info.get(\"source_code\") is not None:\n",
    "            func_codes.append(func_info[\"source_code\"])\n",
    "        if func_info.get(\"doc\") is None:\n",
    "            continue\n",
    "        for key in [\"full\", \"long_description\", \"short_description\"]:\n",
    "            if func_info[\"doc\"].get(key) is not None:\n",
    "                docs.append(f\"{func_name} {func_info['doc'].get(key)}\")\n",
    "                break\n",
    "\n",
    "def file_to_lists(filename):\n",
    "    func_codes = []\n",
    "    docs = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    dic.pop(\"readme_files\", None)\n",
    "    for dir_name, files in dic.items():\n",
    "        for file in files:\n",
    "            if file.get(\"functions\") is not None:\n",
    "                funcs_to_lists(file[\"functions\"], func_codes, docs)\n",
    "            if file.get(\"classes\") is not None:\n",
    "                for class_name, class_info in file[\"classes\"].items():\n",
    "                    if class_info.get(\"methods\") is not None:\n",
    "                        funcs_to_lists(class_info[\"methods\"], func_codes, docs)\n",
    "    return func_codes\n",
    "    \n",
    "repo_info = {}\n",
    "function_list = file_to_lists(f\"content/output/{repo}/directory_info.json\")\n",
    "repo_info[\"funcs\"] = function_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313dfb27-bd9f-48ab-b30a-ca21f0a63ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/codesearch/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"clda/codebert-python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2661d909-e479-4bdd-8cbf-b0b32617002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AutoModel.from_pretrained(\"clda/graphcodebert-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e6ea35-51c3-42ec-9c3b-9896dffa1a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating code embeddings for dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/1171 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1171/1171 [01:22<00:00, 14.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset code embeddings generated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Generating code embeddings for dataset ... \")\n",
    "code_embeddings = []\n",
    "for func in tqdm(repo_info[\"funcs\"]):\n",
    "    code_embeddings.append(model2(tokenizer(func,return_tensors='pt', max_length=512)['input_ids'])[1])\n",
    "    \n",
    "print(\"Dataset code embeddings generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3f4e1f-5bd7-43a5-bb8b-736b734af807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2045482/727774269.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  code_embeddings = torch.stack([torch.tensor(embedding) for embedding in code_embeddings])\n"
     ]
    }
   ],
   "source": [
    "code_embeddings = torch.stack([torch.tensor(embedding) for embedding in code_embeddings])\n",
    "\n",
    "code_vecs = torch.squeeze(code_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e96995d6-f680-4e8b-9994-600c1ea9c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Function to calcualte cosine similarity\n",
      "Code: def cosine_similarity(vec1, vec2):\n",
      "    \"\"\"\n",
      "    Calculate cosine similarity between given two vectors\n",
      "    :type vec1: list\n",
      "    :type vec2: list\n",
      "    \"\"\"\n",
      "    if len(vec1) != len(vec2):\n",
      "        raise ValueError('The two vectors must be the same length. Got shape ' + str(len(vec1)) + ' and ' + str(len(vec2)))\n",
      "    norm_a = _l2_distance(vec1)\n",
      "    norm_b = _l2_distance(vec2)\n",
      "    similarity = 0.0\n",
      "    for (vec1_element, vec2_element) in zip(vec1, vec2):\n",
      "        similarity += vec1_element * vec2_element\n",
      "    similarity /= norm_a * norm_b\n",
      "    return similarity\n",
      "Similarity: 0.9495675563812256\n",
      "Code: def test_cosine_similarity(self):\n",
      "    vec_a = [1, 1, 1]\n",
      "    vec_b = [-1, -1, -1]\n",
      "    vec_c = [1, 2, -1]\n",
      "    self.assertAlmostEqual(cosine_similarity(vec_a, vec_a), 1)\n",
      "    self.assertAlmostEqual(cosine_similarity(vec_a, vec_b), -1)\n",
      "    self.assertAlmostEqual(cosine_similarity(vec_a, vec_c), 0.4714045208)\n",
      "Similarity: 0.0504324734210968\n",
      "Code: def scc(graph):\n",
      "    \"\"\" Computes the strongly connected components of a graph \"\"\"\n",
      "    order = []\n",
      "    visited = {vertex: False for vertex in graph}\n",
      "    graph_transposed = {vertex: [] for vertex in graph}\n",
      "    for (source, neighbours) in graph.iteritems():\n",
      "        for target in neighbours:\n",
      "            add_edge(graph_transposed, target, source)\n",
      "    for vertex in graph:\n",
      "        if not visited[vertex]:\n",
      "            dfs_transposed(vertex, graph_transposed, order, visited)\n",
      "    visited = {vertex: False for vertex in graph}\n",
      "    vertex_scc = {}\n",
      "    current_comp = 0\n",
      "    for vertex in reversed(order):\n",
      "        if not visited[vertex]:\n",
      "            dfs(vertex, current_comp, vertex_scc, graph, visited)\n",
      "            current_comp += 1\n",
      "    return vertex_scc\n",
      "Similarity: 6.6453789138165575e-09\n"
     ]
    }
   ],
   "source": [
    "query = \"Function to calcualte cosine similarity\"\n",
    "query_vec = model2(tokenizer(query,return_tensors='pt')['input_ids'])[1]\n",
    "\n",
    "scores=torch.einsum(\"ab,cb->ac\",query_vec,code_vecs)\n",
    "scores=torch.softmax(scores,-1)\n",
    "\n",
    "top_scores, top_indices = torch.topk(scores[0], k=3, largest=True)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "for score, idx in zip(top_scores, top_indices):\n",
    "    code_embedding = repo_info[\"funcs\"][idx]\n",
    "    print(\"Code:\", code_embedding)\n",
    "    print(\"Similarity:\", score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd990e-7825-4243-b002-7577860a29da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
