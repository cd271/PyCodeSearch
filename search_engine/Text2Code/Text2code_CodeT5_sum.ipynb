{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c897606-892a-4e1d-af44-ec9dda1bc2ee",
   "metadata": {},
   "source": [
    "This notebook implement Text-To-Code Search Engine by code summarization method with `CodeT5` model: create code summarizations for all functions in database, then create embeddings summarizations with  `SentenceTransformers` model and calculate the NL-NL cosine similarity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d67703c6-7e3a-4b39-8c76-090f2d7affff",
   "metadata": {
    "id": "Eh1yfA0-Dt-f"
   },
   "source": [
    "### Download test repositories and run `inspect4py` on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d58ea6-3276-4b02-a712-8bc8b0576c45",
   "metadata": {
    "id": "_G--A3sT61LR"
   },
   "outputs": [],
   "source": [
    "# Repository picked from https://github.com as an example\n",
    "repo = 'keon/algorithms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c7bc956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect4py, version 0.0.6\n"
     ]
    }
   ],
   "source": [
    "!inspect4py --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbd97db-607a-4d11-a24e-bc2bf59e7432",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/Documents/Project/Examples/RepoAnalysis/SemanticCodeSearch/Text2code/content\n",
      "Cloning into 'keon/algorithms'...\n",
      "remote: Enumerating objects: 5162, done.\u001b[K\n",
      "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 5162 (delta 11), reused 16 (delta 3), pack-reused 5136\u001b[K\n",
      "Receiving objects: 100% (5162/5162), 1.42 MiB | 10.99 MiB/s, done.\n",
      "Resolving deltas: 100% (3231/3231), done.\n",
      "Updating files: 100% (477/477), done.\n",
      "Creating jsDir:output/keon/algorithms/algorithms/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/streaming/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/map/json_files\n",
      "Error when processing separate_chaining_hashtable.py:  <class 'AttributeError'>\n",
      "Error when processing hashtable.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/stack/json_files\n",
      "Error when processing stack.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/dfs/json_files\n",
      "Error when processing pacific_atlantic.py:  <class 'AttributeError'>\n",
      "Error when processing walls_and_gates.py:  <class 'AttributeError'>\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/dp/json_files\n",
      "Error when processing longest_increasing.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/search/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/queues/json_files\n",
      "Error when processing priority_queue.py:  <class 'AttributeError'>\n",
      "Error when processing queue.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/greedy/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/json_files\n",
      "Error when processing longest_consecutive.py:  <class 'AttributeError'>\n",
      "Error when processing deepest_left.py:  <class 'AttributeError'>\n",
      "Error when processing invert_tree.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/segment_tree/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/trie/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/traversal/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/fenwick_tree/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/bst/json_files\n",
      "Error when processing kth_smallest.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/avl/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/tree/red_black_tree/json_files\n",
      "Error when processing red_black_tree.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/automata/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/strings/json_files\n",
      "Error when processing word_squares.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/graph/json_files\n",
      "Error when processing path_between_two_vertices_in_digraph.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/set/json_files\n",
      "Error when processing randomized_set.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unionfind/json_files\n",
      "Error when processing count_islands.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/backtrack/json_files\n",
      "Error when processing find_words.py:  <class 'AttributeError'>\n",
      "Error when processing add_operators.py:  <class 'AttributeError'>\n",
      "Error when processing array_sum_combinations.py:  <class 'AttributeError'>\n",
      "Error when processing generate_parenthesis.py:  <class 'AttributeError'>\n",
      "Error when processing palindrome_partitioning.py:  <class 'AttributeError'>\n",
      "Error when processing generate_abbreviations.py:  <class 'AttributeError'>\n",
      "Error when processing combination_sum.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/heap/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unix/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/unix/path/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/ml/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/bfs/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/sort/json_files\n",
      "Error when processing stooge_sort.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/bit/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/compression/json_files\n",
      "Added in funct/method elias_generic , argument named unary, number of argument 0\n",
      "Added in funct/method elias_generic , argument named elias_gamma, number of argument 0\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/linkedlist/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/arrays/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/distribution/json_files\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/matrix/json_files\n",
      "Error when processing search_in_sorted_matrix.py:  <class 'AttributeError'>\n",
      "Error when processing sum_sub_squares.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/algorithms/maths/json_files\n",
      "Error when processing polynomial.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/tests/json_files\n",
      "Error when processing test_monomial.py:  <class 'AttributeError'>\n",
      "Error when processing test_polynomial.py:  <class 'AttributeError'>\n",
      "Creating jsDir:output/keon/algorithms/algorithms/docs/source/json_files\n",
      "Analysis completed\n",
      "Total number of folders processed (root folder is considered a folder): 37\n",
      "Total number of files found:  378\n",
      "Total number of classes found:  269\n",
      "Total number of dependencies found in those files 919\n",
      "Total number of functions parsed:  523\n",
      "/cs/home/cd271/Documents/Project/Examples/RepoAnalysis/SemanticCodeSearch/Text2code\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p content/output\n",
    "%cd content/\n",
    "\n",
    "!mkdir -p {repo} && git clone {f\"https://github.com/{repo}.git\"} {repo}\n",
    "!inspect4py -i {repo} -o output/{repo} -sc -rm\n",
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c21286e-b5d7-47f6-8c73-cd509a458fc3",
   "metadata": {
    "id": "tXCNJRtRKQXP"
   },
   "source": [
    "### Extract docstrings and functions from repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33cf8e0-0e08-4ed9-b857-585eb6e75cb1",
   "metadata": {
    "id": "F-89vQxSFR4s"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def funcs_to_lists(funcs, func_codes, docs):\n",
    "    for func_name, func_info in funcs.items():\n",
    "        if func_info.get(\"source_code\") is not None:\n",
    "            func_codes.append(func_info[\"source_code\"])\n",
    "        if func_info.get(\"doc\") is None:\n",
    "            continue\n",
    "        for key in [\"full\", \"long_description\", \"short_description\"]:\n",
    "            if func_info[\"doc\"].get(key) is not None:\n",
    "                docs.append(f\"{func_name} {func_info['doc'].get(key)}\")\n",
    "                break\n",
    "\n",
    "def file_to_lists(filename):\n",
    "    func_codes = []\n",
    "    docs = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    dic.pop(\"readme_files\", None)\n",
    "    for dir_name, files in dic.items():\n",
    "        for file in files:\n",
    "            if file.get(\"functions\") is not None:\n",
    "                funcs_to_lists(file[\"functions\"], func_codes, docs)\n",
    "            if file.get(\"classes\") is not None:\n",
    "                for class_name, class_info in file[\"classes\"].items():\n",
    "                    if class_info.get(\"methods\") is not None:\n",
    "                        funcs_to_lists(class_info[\"methods\"], func_codes, docs)\n",
    "    return func_codes, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0f0d9b-143b-4ffb-82e7-7437c10cf042",
   "metadata": {
    "id": "zw0cKz1nKXE1"
   },
   "outputs": [],
   "source": [
    "repo_info = {}\n",
    "function_list, _ = file_to_lists(f\"content/output/{repo}/directory_info.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "974b901a",
   "metadata": {},
   "source": [
    "### Using fine-tuned `CodeT5` model from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfffb63c-02df-4384-b41c-3eccb6028d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/home/cd271/codesearch/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"Salesforce/codet5-base-multi-sum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d6429a-b851-41c5-88b8-fc313b2585d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_sum(funcs):\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        funcs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Perform inference to get code similarity\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        outputs = model.generate(**inputs)\n",
    "        \n",
    "    similar_code_snippets = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return similar_code_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b8ab2d-f396-4150-937b-210e2021f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code summaries:   0%|                                                               | 0/1171 [00:00<?, ?it/s]/cs/home/cd271/codesearch/lib64/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Generating code summaries: 100%|████████████████████████████████████████████████████| 1171/1171 [05:36<00:00,  3.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Obtain function_list summarizations \n",
    "code_sum = []\n",
    "with tqdm(total=len(function_list), desc=\"Generating code summaries\") as pbar:\n",
    "    for funcs in function_list:\n",
    "        code_snippets = get_code_sum([funcs])\n",
    "        code_sum.extend(code_snippets)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed2f4c19-ddbe-4a35-a74d-062d7118e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for input query\n",
    "query = ['function to calculate cosine similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee389947-3c76-465b-b06c-505ca17846fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sum_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n",
    "\n",
    "def get_embedding(code_sum, query_sum):\n",
    "    return sum_model.encode(code_sum, convert_to_tensor=True), sum_model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "code_sum_embeddings, query_sum_embedding = get_embedding(code_sum, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "359299ed-cac2-4cbf-856f-5933fd7f02ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "cosine_sim = CosineSimilarity(dim=1)\n",
    "similarities = cosine_sim(query_sum_embedding, code_sum_embeddings).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522c0424-7d2e-4a9f-9835-770359827e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_index(lst, n):\n",
    "    largest_indices = []\n",
    "    for i in range(n):\n",
    "        max_value = max(lst)\n",
    "        max_index = lst.index(max_value)\n",
    "        largest_indices.append(max_index)\n",
    "        lst[max_index] = float('-inf')\n",
    "    return largest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47fc5780-eb32-49ac-9580-0639897c75bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similiar code snippet:\n",
      "\n",
      "Similarity: 0.7125805616378784, \n",
      "def cosine_similarity(vec1, vec2):\n",
      "    \"\"\"\n",
      "    Calculate cosine similarity between given two vectors\n",
      "    :type vec1: list\n",
      "    :type vec2: list\n",
      "    \"\"\"\n",
      "    if len(vec1) != len(vec2):\n",
      "        raise ValueError('The two vectors must be the same length. Got shape ' + str(len(vec1)) + ' and ' + str(len(vec2)))\n",
      "    norm_a = _l2_distance(vec1)\n",
      "    norm_b = _l2_distance(vec2)\n",
      "    similarity = 0.0\n",
      "    for (vec1_element, vec2_element) in zip(vec1, vec2):\n",
      "        similarity += vec1_element * vec2_element\n",
      "    similarity /= norm_a * norm_b\n",
      "    return similarity \n",
      "--------------------------------------------------------------\n",
      "Similarity: 0.621502161026001, \n",
      "def test_cosine_similarity(self):\n",
      "    vec_a = [1, 1, 1]\n",
      "    vec_b = [-1, -1, -1]\n",
      "    vec_c = [1, 2, -1]\n",
      "    self.assertAlmostEqual(cosine_similarity(vec_a, vec_a), 1)\n",
      "    self.assertAlmostEqual(cosine_similarity(vec_a, vec_b), -1)\n",
      "    self.assertAlmostEqual(cosine_similarity(vec_a, vec_c), 0.4714045208) \n",
      "--------------------------------------------------------------\n",
      "Similarity: 0.448973149061203, \n",
      "def combination(n, r):\n",
      "    \"\"\"This function calculates nCr.\"\"\"\n",
      "    if n == r or r == 0:\n",
      "        return 1\n",
      "    return combination(n - 1, r - 1) + combination(n - 1, r) \n",
      "--------------------------------------------------------------\n",
      "Similarity: 0.3899247348308563, \n",
      "def combination_memo(n, r):\n",
      "    \"\"\"This function calculates nCr using memoization method.\"\"\"\n",
      "    memo = {}\n",
      "\n",
      "    def recur(n, r):\n",
      "        if n == r or r == 0:\n",
      "            return 1\n",
      "        if (n, r) not in memo:\n",
      "            memo[n, r] = recur(n - 1, r - 1) + recur(n - 1, r)\n",
      "        return memo[n, r]\n",
      "    return recur(n, r) \n",
      "--------------------------------------------------------------\n",
      "Similarity: 0.3806207776069641, \n",
      "def distance(x, y):\n",
      "    \"\"\"[summary]\n",
      "    HELPER-FUNCTION\n",
      "    calculates the (eulidean) distance between vector x and y.\n",
      "\n",
      "    Arguments:\n",
      "        x {[tuple]} -- [vector]\n",
      "        y {[tuple]} -- [vector]\n",
      "    \"\"\"\n",
      "    assert len(x) == len(y), 'The vector must have same length'\n",
      "    result = ()\n",
      "    sum = 0\n",
      "    for i in range(len(x)):\n",
      "        result += (x[i] - y[i],)\n",
      "    for component in result:\n",
      "        sum += component ** 2\n",
      "    return math.sqrt(sum) \n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sim = similarities.copy()\n",
    "index = find_top_n_index(sim,5)\n",
    "print('Similiar code snippet:\\n')\n",
    "for i in index:\n",
    "    print(f'Similarity: {similarities[i]}, \\n{function_list[i]} \\n--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741513a2-caef-4d0e-a742-291e0f6edc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
